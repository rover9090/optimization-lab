
# Cross-Database E-commerce Reporting System

A high-performance reporting module built with Laravel, designed to handle large-scale datasets (100,000+ records) across distributed database architectures. This project demonstrates the evolution from traditional SQL joins to scalable **Application-side Joins**.

## üöÄ Architectural Overview

In this project, I manage data across two distinct database instances:

1. **Orders Database**: Stores transactional data (`orders`, `order_lines`, `product_data`).
2. **Middleware Database**: Stores global configurations (`website_config`) including regional locales and country mappings.

### The Challenge

Performing standard SQL `JOIN` operations across different database connections (or physical servers) introduces significant bottlenecks, including high latency, tight coupling, and database CPU contention.

---

## üõ† Optimization Strategy

I implemented two distinct phases of optimization to demonstrate technical maturity and scalability planning.

### Phase 1: The Baseline (Cross-Database Join)

Initially, the report was generated using a standard SQL Join. While functional, this approach:

* Constrains the databases to the same physical host.
* Increases row scan counts significantly during aggregation.
* Couplings the "Orders" service directly to the "Middleware" configuration schema.

### Phase 2: Application-side Join (Current Implementation)

To prepare for a distributed/microservices environment, I refactored the logic to use an **Application-side Join**.

* **Logic**: Fetch configuration metadata into a keyed collection first, then query the transaction data using a `whereIn` filter based on the retrieved locales.
* **Benefits**:
* **Decoupling**: The databases can now live on separate servers.
* **Performance**: Replaced expensive SQL join logic with  memory lookups in PHP.
* **Stability**: Avoids long-running table locks on the configuration database.

* **‚úÖ Pros:**
* **Zero Schema Changes**: No risk of table locks; 100% safe to deploy on legacy production databases.
* **High Flexibility**: Easily handles changes in business logic (e.g., re-mapping countries) without migrating millions of rows.
* **Distributed Ready**: The `Orders` and `Middleware` databases can be physically separated, making this architecture **Microservices-ready**.


* **‚ùå Cons:**
* **Memory Usage**: Requires holding a small configuration mapping in PHP memory (negligible for country/locale data).
* **Slightly Higher Latency**: Requires two separate database queries instead of one.


---

## üìà Performance & Scalability

| Metric | Original (SQL Join) | Optimized (App-side Join) |
| --- | --- | --- |
| **Complexity** |  (DB Level) |  (Memory Level) |
| **DB Coupling** | Tight (Same Host Required) | Loose (Distributed Friendly) |
| **Scalability** | Limited by DB CPU | Horizontally Scalable (App Level) |

### Future Evolution (Phase 3: Denormalization)

For systems exceeding 10M+ rows, I have designed a **Data Patching Migration**. This phase would involve denormalizing `country_code` directly into the `orders` table. While this introduces data redundancy, it provides the ultimate performance by enabling pure single-table indexing.

*Note: This was intentionally kept as a "Phase 3" plan to avoid premature optimization and maintain data integrity at current scales.*

* **‚úÖ Pros:**
* **Maximum Query Performance**: Enables pure single-table scans with optimized indexes.
* **Simplified SQL**: Eliminates all logic-based filtering in PHP.


* **‚ùå Cons:**
* **Data Integrity Risks**: Requires complex observers or triggers to keep redundant data synced across tables (e.g., if a locale‚Äôs country changes).
* **Storage Overhead**: Increases database size as strings are duplicated millions of times.
* **Deployment Risks**: Adding columns to a table with 10M+ rows causes **long table locks**, potentially leading to production downtime.

---

## üíª Technical Stack

* **Framework**: Laravel 11
* **Database**: MySQL (Dual-connection setup)
* **Techniques**: Eloquent Relationships, Custom Artisan Commands, Data Patching Migrations, Defensive Programming.

## üèÉ How to Run

1. Ensure both database connections are configured in `.env`.
2. Run migrations: `php artisan migrate`.
3. Generate the report:
```bash
# Standard Report (Original Logic)
php artisan report:regional-sales ca

# Optimized Report (Application-side Join)
php artisan report:regional-sales ca --optimized
```
---

### üß† Engineering Q&A (Technical Deep Dive)

This section documents the engineering considerations and trade-offs made during the development of this reporting system.

#### **Q1: How do you handle failures during a large-scale data patching (e.g., 100k+ rows)?**

**A:** I prioritize **Idempotency** and **Resumability**.

* **Idempotency**: The patching scripts are designed to check if a record is already updated before performing a write operation. This prevents data corruption if the script needs to be restarted.
* **Transactional Integrity**: Updates are wrapped in small database transactions. If a specific chunk fails, only that batch is rolled back, leaving the rest of the migrated data intact.
* **Monitoring**: I implement logging for failed records, allowing for targeted re-runs instead of full-table rollbacks.

#### **Q2: How do you prevent table locking and performance degradation in production?**

**A:** I utilize **Low-Priority Batch Processing**.

* **Chunking**: Instead of a single `UPDATE` statement that locks the entire table, I use Laravel‚Äôs `chunkById` to process 1,000 rows at a time.
* **Throttle/Sleep**: In a live environment, I introduce short sleep intervals (e.g., 100ms) between batches to allow the DB engine to process concurrent user transactions (like new orders).
* **Off-Peak Execution**: While the script is optimized for performance, I still schedule such migrations during off-peak hours to minimize the blast radius of any potential latency spikes.

#### **Q3: Why not use a standard SQL Cross-Database Join?**

**A:** To ensure **Architectural Decoupling** and **Scalability**.

* Standard Joins require both databases to reside on the same physical instance. By implementing an **Application-side Join**, I allow the "Orders" and "Config" services to scale independently.
* This approach is **Microservices-ready**, meaning the system can easily transition to a distributed cloud environment where databases are hosted on different clusters.

#### **Q4: In a distributed setup, how do you ensure Unique IDs across different databases?**

**A:** For distributed systems, I recommend the **Snowflake Algorithm** or **ULIDs**.

* Traditional `AUTO_INCREMENT` leads to ID collisions when sharding data.
* **Snowflake IDs** provide a 64-bit trend-increasing integer that includes a `Timestamp`, `Worker ID`, and `Sequence`. This guarantees uniqueness across nodes and maintains high indexing performance in MySQL's B+Tree structure.
